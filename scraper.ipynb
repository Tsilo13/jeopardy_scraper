{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests  # For making HTTP requests\n",
    "from bs4 import BeautifulSoup  # For parsing HTML\n",
    "import json  # For storing scraped data\n",
    "import os  # For file operations\n",
    "import time  # To add delays between requests (avoid being blocked)\n",
    "\n",
    "URL = \"https://www.j-archive.com/showseason.php?season=33\"\n",
    "page = requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "season_episodes_URL = base_url + \"showseason.php?season=33\"\n",
    "season_episodes_page = requests.get(season_episodes_URL)\n",
    "soup = BeautifulSoup(season_episodes_page.content, \"html.parser\")\n",
    "\n",
    "episodes_json_array = []\n",
    "episodes = soup.find_all(\"tr\")\n",
    "for episode in episodes:\n",
    "    # init new episode json object\n",
    "    episode_json = {\"id\": None, \"name\": None, \"air_date\": None, \"URL\": None}\n",
    "\n",
    "    # get 'a' tag for ID, air date, and URL\n",
    "    a_tag = episode.find(\"a\")\n",
    "    episode_json[\"id\"] = a_tag.string.split(\",\")[0].lstrip(\"#\")\n",
    "    episode_json[\"air_date\"] = a_tag.string[-10:]\n",
    "    episode_json[\"URL\"] = base_url + a_tag[\"href\"]\n",
    "\n",
    "    # get 'td' tag for episode name\n",
    "    td = episode.find_all('td')[1]\n",
    "    string = td.string.lstrip().rstrip()\n",
    "    episode_json[\"name\"] = string\n",
    "\n",
    "    # append episode json to array\n",
    "    episodes_json_array.append(episode_json)\n",
    "\n",
    "print(episodes_json_array)\n",
    "\n",
    "for episode in episodes_json_array:\n",
    "    episode_url = episode[\"URL\"]\n",
    "    file_path = f\"data/episod_{game_id}.json\"\n",
    "    \n",
    "    # make request before status check\n",
    "    try:\n",
    "        response = requests.get(episode_url, timeout=5)  # Set timeout\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for {episode_url}: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    if response.status_code != 200: # status check\n",
    "        print(f\"HTTP request unsuccessful for {episode_url}\")\n",
    "        continue # skips to next episode\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(episode_data, f, indent=4)\n",
    "        print(f\"Saved episode {game_id} to {file_path}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")  # Parse the HTML\n",
    "\n",
    "    jeopardy_round = soup.find(\"div\", id=\"jeopardy_round\")\n",
    "    double_jeopardy_round = soup.find(\"div\", id=\"double_jeopardy_round\")\n",
    "    final_jeopardy_round = soup.find(\"div\", id=\"final_jeopardy_round\")\n",
    "\n",
    "    single_categories = [cat.text.strip() for cat in jeopardy_round.find_all(\"td\", class_=\"category_name\")]\n",
    "    double_categories = [cat.text.strip() for cat in double_jeopardy_round.find_all(\"td\", class_=\"category_name\")]\n",
    "    final_category = [cat.text.strip() for cat in final_jeopardy_round.find_all(\"td\", class_=\"category_name\")]\n",
    "    clues = soup.find_all(\"td\", class_=\"clue_text\")\n",
    "\n",
    "    # Initialize game_data\n",
    "    game_data = {\"Jeopardy Round\": {}}\n",
    "\n",
    "    # Helper function to parse clue IDs\n",
    "    def parse_clue_id(clue_id):\n",
    "        parts = clue_id.split(\"_\")\n",
    "        if len(parts) < 4:\n",
    "            return None, None\n",
    "        return int(parts[2]), int(parts[3])  # Extract X (category) and Y (row)\n",
    "\n",
    "    # Loop through all clues\n",
    "    for clue in soup.find_all(\"td\", class_=\"clue_text\"):\n",
    "        clue_id = clue.get(\"id\", \"No ID\")\n",
    "        clue_text = clue.text.strip()\n",
    "\n",
    "        # Parse clue ID to get category (X) and row index (Y)\n",
    "        X, Y = parse_clue_id(clue_id)\n",
    "        if X is None or Y is None:\n",
    "            continue  # Skip invalid clues\n",
    "\n",
    "        # Get the actual category name\n",
    "        category = single_categories[X - 1] if 1 <= X <= len(single_categories) else \"Unknown Category\"\n",
    "\n",
    "        # Generate the corresponding answer ID\n",
    "        answer_id = clue_id.replace(\"clue_\", \"clue_\", 1) + \"_r\"\n",
    "        answer_tag = soup.find(id=answer_id)\n",
    "        answer = answer_tag.find(\"em\", class_=\"correct_response\").text.strip() if answer_tag else \"No Answer\"\n",
    "\n",
    "        # Ensure category exists in game_data\n",
    "        if category not in game_data[\"Jeopardy Round\"]:\n",
    "            game_data[\"Jeopardy Round\"][category] = []\n",
    "\n",
    "        # Store the clue in game_data\n",
    "        game_data[\"Jeopardy Round\"][category].append({\n",
    "            \"value\": Y * 200,  # Convert row index to clue value\n",
    "            \"question\": clue_text,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
